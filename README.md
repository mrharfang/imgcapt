# IMGCAPT: FLUX LoRA Training Dataset Preparation Tool\n\n**Professional image captioning and dataset preparation for FLUX model training.**\n\nIMGCAPT is a complete solution for preparing high-quality image datasets for FLUX LoRA training. It combines AI-powered caption generation with professional manual editing tools, following research-backed captioning strategies for optimal style learning.\n\n![IMGCAPT Interface](https://img.shields.io/badge/Interface-Professional-blue) ![AI-Powered](https://img.shields.io/badge/AI-Ollama%20LLaVA-green) ![FLUX-Optimized](https://img.shields.io/badge/FLUX-Optimized-orange)\n\n## ‚ú® Features\n\n### ü§ñ AI-Powered Caption Generation\n- **Ollama LLaVA Integration**: Leverages LLaVA 7B vision model for intelligent image analysis\n- **FLUX-Optimized Prompts**: Research-backed prompting strategy for style training\n- **Factual Descriptions**: Generates clean, objective descriptions without style bias\n- **Batch Processing**: Auto-caption entire datasets with progress tracking\n\n### üé® Professional Editing Interface\n- **Dual-Mode UI**: Separate workflows for new images and dataset management\n- **Canvas-Based Cropping**: Precision image cropping with aspect ratio controls (16:9, 1:1)\n- **Real-Time Preview**: Instant visual feedback during editing\n- **Auto-Save**: Seamless caption editing with automatic persistence\n\n### üìÅ Dataset Management\n- **Paired File System**: Automatic PNG/TXT file synchronization\n- **Backup Protection**: Automatic caption backups during batch processing\n- **Import Workflow**: Drag-and-drop folder import with progress tracking\n- **Navigation Controls**: Keyboard shortcuts and intuitive browsing\n\n### üîÑ Real-Time Updates\n- **Server-Sent Events**: Live progress tracking for all operations\n- **Connection Resilience**: Auto-reconnecting event stream\n- **Detailed Logging**: Comprehensive operation logs with timestamps\n\n## üöÄ Quick Start\n\n### Prerequisites\n- **Python 3.8+** with UV package manager\n- **Ollama** with LLaVA model installed\n- **Modern web browser**\n\n### Installation\n\n1. **Clone and setup**:\n   ```bash\n   git clone https://github.com/yourusername/imgcapt.git\n   cd imgcapt\n   uv venv\n   source .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install dependencies**:\n   ```bash\n   uv sync\n   # OR if you prefer pip:\n   pip install -r requirements.txt\n   ```\n\n3. **Setup Ollama**:\n   ```bash\n   # Install Ollama (if not already installed)\n   curl -fsSL https://ollama.ai/install.sh | sh\n   \n   # Pull LLaVA model\n   ollama pull llava:7b\n   \n   # Start Ollama service\n   ollama serve\n   ```\n\n4. **Launch IMGCAPT**:\n   ```bash\n   python backend/main.py\n   ```\n\n5. **Open your browser**:\n   Navigate to `http://localhost:8000`\n\n## üìñ Usage Guide\n\n### Import Images\n1. Click the **folder button** in the Input tab\n2. Select a folder containing your training images\n3. Wait for import completion (progress shown in real-time)\n\n### AI Caption Generation\n1. Select an image from the file list\n2. Adjust cropping and aspect ratio as needed\n3. Click **\"GENERATE CAPTION\"** for AI analysis\n4. Review and edit the generated caption\n5. Click **\"PROCESS\"** to save to dataset\n\n### Dataset Management\n1. Switch to the **Processed tab**\n2. Navigate through your dataset using arrow keys or buttons\n3. Edit captions directly with auto-save\n4. Delete unwanted entries with confirmation\n\n### Batch Processing\nFor bulk caption regeneration:\n```bash\npython batch_recaption.py\n```\nThis will:\n- Backup existing captions with `BKUP_` prefix\n- Generate new AI captions for all images\n- Maintain file organization\n\n## üß† Caption Strategy\n\n### FLUX Training Optimization\nBased on extensive research into FLUX LoRA training, IMGCAPT uses a **style-focused captioning approach**:\n\n**‚úÖ What We Caption:**\n- People: actions, expressions, demographics, clothing\n- Setting: location, environment, objects, furniture\n- Composition: angles, framing, depth, perspective\n- Technical: lighting quality, camera perspective\n\n**‚ùå What We Avoid:**\n- Artistic style or mood descriptions\n- Editorial/magazine-like qualities\n- Specialized terminology\n- Religious or cultural context (unless explicit)\n\n### Research Foundation\nOur approach follows findings from the FLUX training community:\n- **Consistent datasets** with factual descriptions train better than complex captioning\n- **Style separation** from content allows longer training without overfitting\n- **Generic vocabulary** prevents model confusion during generation\n\n## üèóÔ∏è Architecture\n\n### Backend (FastAPI)\n- **Modern async Python** with type hints throughout\n- **Ollama integration** via REST API\n- **Server-Sent Events** for real-time updates\n- **Robust error handling** with detailed logging\n\n### Frontend (Vanilla JS)\n- **No framework dependencies** - pure JavaScript\n- **Responsive design** with professional UI\n- **Canvas-based editing** for precise image control\n- **Real-time communication** via SSE\n\n### File Structure\n```\nimgcapt/\n‚îú‚îÄ‚îÄ backend/\n‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI server\n‚îÇ   ‚îî‚îÄ‚îÄ sse_manager.py       # Real-time event handling\n‚îú‚îÄ‚îÄ frontend/\n‚îÇ   ‚îú‚îÄ‚îÄ index.html           # Main interface\n‚îÇ   ‚îú‚îÄ‚îÄ app.js              # Core application logic\n‚îÇ   ‚îú‚îÄ‚îÄ scripts/            # Modular components\n‚îÇ   ‚îî‚îÄ‚îÄ style.css           # Professional styling\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Imported images\n‚îÇ   ‚îî‚îÄ‚îÄ processed/          # Training dataset (PNG + TXT pairs)\n‚îú‚îÄ‚îÄ batch_recaption.py      # Bulk processing script\n‚îî‚îÄ‚îÄ requirements.txt        # Python dependencies\n```\n\n## ‚öôÔ∏è Configuration\n\n### Environment Variables\nCreate `.env` file for custom configuration:\n```env\nPORT=8000\nHOST=0.0.0.0\nOLLAMA_URL=http://localhost:11434\n```\n\n### Ollama Models\nSupported vision models:\n- `llava:7b` (recommended) - Good balance of speed/quality\n- `llava:13b` - Higher quality, slower processing\n- `llava:34b` - Best quality, requires significant VRAM\n\n## üîß Advanced Usage\n\n### Custom Prompts\nModify the vision prompt in `backend/main.py` around line 570 to customize caption style for your specific use case.\n\n### Batch Configuration\nEdit `batch_recaption.py` to:\n- Change the Ollama model\n- Modify processing parameters\n- Add custom vocabulary mapping\n\n### API Integration\nThe FastAPI backend exposes RESTful endpoints:\n- `POST /api/generate-caption` - AI caption generation\n- `GET /api/processed-images` - Dataset listing\n- `PUT /api/processed-caption/{id}` - Caption updates\n- `GET /api/sse/events` - Real-time event stream\n\n## ü§ù Contributing\n\nWe welcome contributions! Please:\n\n1. **Fork the repository**\n2. **Create a feature branch**: `git checkout -b feature/amazing-feature`\n3. **Commit changes**: `git commit -m 'Add amazing feature'`\n4. **Push to branch**: `git push origin feature/amazing-feature`\n5. **Open a Pull Request**\n\n### Development Setup\n```bash\n# Install in development mode\nuv pip install -e .\n\n# Run with auto-reload\nuvicorn backend.main:app --reload --host 0.0.0.0 --port 8000\n```\n\n## üìù License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## üôè Acknowledgments\n\n- **Ollama Team** for the excellent local LLM runtime\n- **LLaVA Project** for the powerful vision-language model\n- **FLUX Community** for sharing training insights and best practices\n- **FastAPI** for the robust async web framework\n\n## üìû Support\n\n- **Issues**: [GitHub Issues](https://github.com/yourusername/imgcapt/issues)\n- **Discussions**: [GitHub Discussions](https://github.com/yourusername/imgcapt/discussions)\n- **Documentation**: See `/docs` folder for detailed guides\n\n---\n\n**Built with ‚ù§Ô∏è for the AI training community**\n\n*Empowering creators to build better datasets, one caption at a time.*"